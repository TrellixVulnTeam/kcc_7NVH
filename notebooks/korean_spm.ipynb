{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sentencepiece as spm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"../../korean/naver.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Compelte: naver model & vocab\n",
      "Make Compelte: naver naver tokenized data\n",
      "Saving Tokenized Data is Done!\n",
      "Train Compelte: naver model & vocab\n",
      "Make Compelte: naver naver tokenized data\n",
      "Saving Tokenized Data is Done!\n"
     ]
    }
   ],
   "source": [
    "data_type = 'naver'\n",
    "tokenized_data = {}\n",
    "tokenized_data[data_type] = {}\n",
    "# tokenized_data[data_type] = {}\n",
    "# tokenized_data[corpus]['test'] = {}\n",
    "\n",
    "train_input_file = f\"../../korean/naver.txt\"\n",
    "\n",
    "parameter = '--input={} \\\n",
    "--pad_id={} --pad_piece={} \\\n",
    "--bos_id={} --bos_piece={} \\\n",
    "--eos_id={} --eos_piece={} \\\n",
    "--unk_id={} --unk_piece={} \\\n",
    "--user_defined_symbols={} \\\n",
    "--model_prefix={} \\\n",
    "--vocab_size={} \\\n",
    "--max_sentence_length={} \\\n",
    "--character_coverage={} \\\n",
    "--model_type={}'\n",
    "\n",
    "pad_id = 0\n",
    "pad_piece = \"[PAD]\"\n",
    "bos_id = 1\n",
    "bos_piece = \"[BOS]\"\n",
    "eos_id = 2\n",
    "eos_piece = \"[EOS]\"\n",
    "unk_id = 3\n",
    "unk_piece = \"[UNK]\"\n",
    "user_defined_symbols = \"[SEP],[CLS],[MASK]\"\n",
    "vocab_size = 16000\n",
    "max_sentence_length = 160 # naver max_length 160\n",
    "character_coverage = 0.9995  # default\n",
    "m_types = ['unigram', 'bpe'] \n",
    "for m_type in m_types:\n",
    "    model_type = m_type # default: unigram\n",
    "    prefix = (train_input_file.split(\"/\")[-1]).split('.')[0] #naver\n",
    "    model_prefix = f'../data/tokenizer/{prefix}_{m_type}' #naver_unigram / naver_bpe\n",
    "#     print(model_prefix)\n",
    "\n",
    "    cmd = parameter.format(train_input_file,\n",
    "                           pad_id, pad_piece,\n",
    "                           bos_id, bos_piece,\n",
    "                           eos_id, eos_piece,\n",
    "                           unk_id, unk_piece,\n",
    "                           user_defined_symbols,\n",
    "                           model_prefix,\n",
    "                           vocab_size,\n",
    "                           max_sentence_length,\n",
    "                           character_coverage,\n",
    "                           model_type)\n",
    "    spm.SentencePieceProcessor()\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "    print(f\"Train Compelte: {prefix} model & vocab\")\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(f\"{model_prefix}.model\")\n",
    "\n",
    "    # BOS, EOS 추가\n",
    "    sp.SetEncodeExtraOptions('bos:eos')\n",
    "\n",
    "    # Tokenization And Padding\n",
    "    with open(train_input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokenized_data[data_type][prefix] = [sp.EncodeAsIds(line) for line in f]\n",
    "        print(f\"Make Compelte: {data_type} {prefix} tokenized data\")\n",
    "\n",
    "    # Save Data\n",
    "    processed_path = \"../data/processed/tokenized/{data_type}.pkl\"\n",
    "    with open(processed_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_data, file)\n",
    "    print(\"Saving Tokenized Data is Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Compelte: hate model & vocab\n",
      "Make Compelte: naver hate tokenized data\n",
      "Saving Tokenized Data is Done!\n",
      "Train Compelte: hate model & vocab\n",
      "Make Compelte: naver hate tokenized data\n",
      "Saving Tokenized Data is Done!\n"
     ]
    }
   ],
   "source": [
    "data_type = 'naver'\n",
    "tokenized_data = {}\n",
    "tokenized_data[data_type] = {}\n",
    "# tokenized_data[data_type] = {}\n",
    "# tokenized_data[corpus]['test'] = {}\n",
    "\n",
    "train_input_file = f\"../../korean/hate.txt\"\n",
    "\n",
    "parameter = '--input={} \\\n",
    "--pad_id={} --pad_piece={} \\\n",
    "--bos_id={} --bos_piece={} \\\n",
    "--eos_id={} --eos_piece={} \\\n",
    "--unk_id={} --unk_piece={} \\\n",
    "--user_defined_symbols={} \\\n",
    "--model_prefix={} \\\n",
    "--vocab_size={} \\\n",
    "--max_sentence_length={} \\\n",
    "--character_coverage={} \\\n",
    "--model_type={}'\n",
    "\n",
    "pad_id = 0\n",
    "pad_piece = \"[PAD]\"\n",
    "bos_id = 1\n",
    "bos_piece = \"[BOS]\"\n",
    "eos_id = 2\n",
    "eos_piece = \"[EOS]\"\n",
    "unk_id = 3\n",
    "unk_piece = \"[UNK]\"\n",
    "user_defined_symbols = \"[SEP],[CLS],[MASK]\"\n",
    "vocab_size = 13000\n",
    "max_sentence_length = 140 # hate max_length 160\n",
    "character_coverage = 0.9995  # default\n",
    "m_types = ['unigram', 'bpe'] \n",
    "for m_type in m_types:\n",
    "    model_type = m_type # default: unigram\n",
    "    prefix = (train_input_file.split(\"/\")[-1]).split('.')[0] #naver\n",
    "    model_prefix = f'../data/tokenizer/{prefix}_{m_type}' #naver_unigram / naver_bpe\n",
    "#     print(model_prefix)\n",
    "\n",
    "    cmd = parameter.format(train_input_file,\n",
    "                           pad_id, pad_piece,\n",
    "                           bos_id, bos_piece,\n",
    "                           eos_id, eos_piece,\n",
    "                           unk_id, unk_piece,\n",
    "                           user_defined_symbols,\n",
    "                           model_prefix,\n",
    "                           vocab_size,\n",
    "                           max_sentence_length,\n",
    "                           character_coverage,\n",
    "                           model_type)\n",
    "    spm.SentencePieceProcessor()\n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "    print(f\"Train Compelte: {prefix} model & vocab\")\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(f\"{model_prefix}.model\")\n",
    "\n",
    "    # BOS, EOS 추가\n",
    "    sp.SetEncodeExtraOptions('bos:eos')\n",
    "\n",
    "    # Tokenization And Padding\n",
    "    with open(train_input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokenized_data[data_type][prefix] = [sp.EncodeAsIds(line) for line in f]\n",
    "        print(f\"Make Compelte: {data_type} {prefix} tokenized data\")\n",
    "\n",
    "    # Save Data\n",
    "    processed_path = \"../data/processed/tokenized/{data_type}.pkl\"\n",
    "    with open(processed_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_data, file)\n",
    "    print(\"Saving Tokenized Data is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

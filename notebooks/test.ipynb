{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class StyleTransfer(nn.Module):\n",
    "    def __init__(self, encoder, tst_decoder, d_hidden, style_ratio, variational, device):\n",
    "        super(StyleTransfer, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.tst_decoder = tst_decoder\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.style_ratio = style_ratio\n",
    "        self.content_index = int(self.d_hidden * (1 - self.style_ratio))\n",
    "        self.style_index = int(self.d_hidden-self.content_index)\n",
    "\n",
    "        self.variational = variational\n",
    "\n",
    "\n",
    "        # TODO Size ?\n",
    "        self.half_hidden = nn.Linear(4, 2)\n",
    "        self.content2mean = nn.Linear(self.content_index, d_hidden)\n",
    "        self.content2logv = nn.Linear(self.content_index, d_hidden)\n",
    "\n",
    "        self.style2mean = nn.Linear(self.style_index, d_hidden)\n",
    "        self.style2logv = nn.Linear(self.style_index, d_hidden)\n",
    "\n",
    "    def reparameterization(self, hidden, latent_type):\n",
    "        hidden = hidden.transpose(0, -1)\n",
    "        hidden = self.half_hidden(hidden)\n",
    "\n",
    "        hidden = hidden.transpose(0, -1)\n",
    "        if latent_type == \"content\":\n",
    "            mean = self.content2mean(hidden).to(self.device)\n",
    "            logv = self.content2logv(hidden).to(self.device)\n",
    "\n",
    "        elif latent_type == \"style\":\n",
    "            mean = self.style2mean(hidden).to(self.device)\n",
    "            logv = self.style2logv(hidden).to(self.device)\n",
    "\n",
    "        std = torch.exp(0.5 * logv)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + (eps * std)\n",
    "        return z, mean, logv\n",
    "\n",
    "    def forward(self, tst_src, tst_trg, teacher_forcing_ratio=0.5):\n",
    "        tst_src = tst_src.to(self.device)\n",
    "        tst_trg = tst_trg.to(self.device)\n",
    "\n",
    "        encoder_out, hidden, cell = self.encoder(tst_src)\n",
    "\n",
    "        if self.variational:\n",
    "            context_c, context_a = hidden[:, :, :self.content_index], hidden[:, :, -self.style_index:]\n",
    "\n",
    "            # TODO 따로 따로 reparameterize? 아니면 reparameterize 한 다음에 split?\n",
    "            # TODO 나눈 후 size 맞추기 위해 content 밑에/style 위에 0으로 채워서 reparameterize?\n",
    "            content_c, content_mu, content_logv = self.reparameterization(context_c, \"content\")\n",
    "            style_a, style_mu, style_logv = self.reparameterization(context_a, \"style\")\n",
    "\n",
    "            total_latent = torch.cat((content_c, style_a), 0)\n",
    "\n",
    "            # TODO cat? add? -> 일단은 total_latent로 진행\n",
    "            hidden = total_latent\n",
    "\n",
    "            latent_variables = [total_latent, content_c, content_mu, content_logv, style_a, style_mu, style_logv,]\n",
    "\n",
    "        else:\n",
    "            latent_variables = [None for _ in range(7)]\n",
    "\n",
    "        trg_len = tst_trg.shape[1]  # length of word\n",
    "        batch_size = tst_trg.shape[0]  # batch size\n",
    "        trg_vocab_size = self.tst_decoder.output_size\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        input = tst_trg[:, 0]  # BOS 먼저\n",
    "\n",
    "        output_list = []\n",
    "        for i in range(1, trg_len):\n",
    "            output, hidden, cell = self.tst_decoder(input, hidden, cell)\n",
    "            outputs[:, i] = output\n",
    "            output_list.append(torch.argmax(output, dim=1).tolist())\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = tst_trg[:, i] if teacher_force else top1\n",
    "\n",
    "        return outputs, latent_variables, output_list\n",
    "\n",
    "class StylizedNMT(nn.Module):\n",
    "    def __init__(self, nmt_encoder, nmt_decoder, d_hidden, total_latent, device):\n",
    "        super(StylizedNMT, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.nmt_encoder = nmt_encoder\n",
    "        self.nmt_decoder = nmt_decoder\n",
    "        self.total_latent = total_latent\n",
    "\n",
    "        self.hidden2concat = nn.Linear(d_hidden, d_hidden // 2)\n",
    "        self.latent2concat = nn.Linear(d_hidden, d_hidden // 2)\n",
    "\n",
    "    def forward(self, nmt_src, nmt_trg, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        # nmt_hidden = nmt_hidden.to(self.device)\n",
    "        # nmt_cell = nmt_cell.to(self.device)\n",
    "        nmt_src = nmt_src.to(self.device)\n",
    "        nmt_trg = nmt_trg.to(self.device)\n",
    "\n",
    "        encoder_out, hidden, cell = self.encoder(nmt_src)\n",
    "\n",
    "        # TODO add 할 지, concat 할 지\n",
    "        if self.total_latent:\n",
    "            hidden = self.hidden2concat(hidden)\n",
    "            latent = self.latent2concat(self.total_latent)\n",
    "            hidden = torch.cat((hidden, latent), 2)\n",
    "\n",
    "        trg_len = nmt_trg.shape[1]  # length of word\n",
    "        batch_size = nmt_trg.shape[0]  # batch size\n",
    "        trg_vocab_size = self.nmt_decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        input = nmt_trg[:, 0]\n",
    "\n",
    "        output_list = []\n",
    "        for i in range(1, trg_len):\n",
    "            output, hidden, cell = self.nmt_decoder(input, hidden, cell)\n",
    "            outputs[:, i] = output\n",
    "            output_list.append(torch.argmax(output, dim=1).tolist())\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = nmt_trg[:, i] if teacher_force else top1\n",
    "\n",
    "        return outputs, output_list\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, d_hidden, d_embed, n_layers, dropout, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(input_size, d_embed)\n",
    "\n",
    "        # TODO num_layers=2 -> total_latent [8, batch_size, d_hidden] 이거 어떻게 해결?\n",
    "        self.encoder = nn.LSTM(input_size=d_embed, hidden_size=d_hidden, dropout=dropout,\n",
    "                               num_layers=n_layers, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.src_embedding(src))\n",
    "        outputs, (hidden, cell) = self.encoder(embedded)\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class TSTDecoder(nn.Module):\n",
    "    def __init__(self, output_size, d_hidden, d_embed, n_layers, dropout, device):\n",
    "        super(TSTDecoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.trg_embedding = nn.Embedding(output_size, d_embed)\n",
    "        self.tst_decoder = nn.LSTM(input_size=d_embed, hidden_size=d_hidden, dropout=dropout,\n",
    "                                   num_layers=n_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2*d_hidden, output_size)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.trg_embedding(input))\n",
    "\n",
    "        outputs, (hidden, cell) = self.tst_decoder(embedded, (hidden, cell))\n",
    "\n",
    "        tst_out = self.fc(outputs.squeeze(1))\n",
    "\n",
    "        return tst_out, hidden, cell\n",
    "\n",
    "\n",
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, output_size, d_hidden, d_embed, n_layers, dropout, device):\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.trg_embedding = nn.Embedding(output_size, d_embed)\n",
    "        self.nmt_decoder = nn.LSTM(input_size=d_embed, hidden_size=d_hidden, dropout=dropout,\n",
    "                                   num_layers=n_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2*d_hidden, output_size)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.trg_embedding(input))\n",
    "        outputs, (hidden, cell) = self.nmt_decoder(embedded, (hidden, cell))\n",
    "        nmt_out = self.fc(outputs.squeeze(1))\n",
    "\n",
    "        return nmt_out, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': OrderedDict([('nmt_decoder.trg_embedding.weight',\n",
       "               tensor([[-0.1841, -2.4435,  0.0471,  ..., -0.2534, -2.0047, -2.5667],\n",
       "                       [-0.4562, -0.3122,  1.0801,  ..., -0.6673,  0.2852, -0.0731],\n",
       "                       [-0.8661,  0.1094, -0.9197,  ..., -0.0761, -0.5270, -0.8286],\n",
       "                       ...,\n",
       "                       [ 1.0204, -0.6731, -0.3677,  ...,  1.0387,  1.6615, -0.9600],\n",
       "                       [ 1.5721,  0.0214,  0.0233,  ..., -2.1208,  0.2514, -1.2886],\n",
       "                       [-0.4011, -0.2870,  0.5079,  ..., -0.0361, -0.6963,  0.2815]])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_ih_l0',\n",
       "               tensor([[ 0.0287, -0.0170,  0.0073,  ..., -0.0110,  0.0170,  0.0187],\n",
       "                       [-0.0019, -0.0304, -0.0227,  ...,  0.0002, -0.0269,  0.0180],\n",
       "                       [-0.0213,  0.0008,  0.0090,  ..., -0.0245, -0.0202,  0.0015],\n",
       "                       ...,\n",
       "                       [-0.0207, -0.0305, -0.0227,  ...,  0.0117, -0.0243, -0.0003],\n",
       "                       [ 0.0121,  0.0030,  0.0138,  ..., -0.0269, -0.0287,  0.0203],\n",
       "                       [-0.0011, -0.0058, -0.0044,  ..., -0.0153, -0.0200, -0.0010]])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_hh_l0',\n",
       "               tensor([[ 0.0075,  0.0133, -0.0131,  ..., -0.0242,  0.0229,  0.0106],\n",
       "                       [ 0.0271, -0.0128, -0.0229,  ...,  0.0180, -0.0167,  0.0278],\n",
       "                       [-0.0096, -0.0168, -0.0209,  ...,  0.0013,  0.0246, -0.0268],\n",
       "                       ...,\n",
       "                       [-0.0240,  0.0094, -0.0277,  ..., -0.0115,  0.0069, -0.0025],\n",
       "                       [-0.0022,  0.0188, -0.0024,  ..., -0.0258, -0.0219,  0.0095],\n",
       "                       [-0.0273,  0.0303,  0.0262,  ..., -0.0062, -0.0044,  0.0033]])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_ih_l0',\n",
       "               tensor([-0.0081,  0.0226, -0.0109,  ...,  0.0194,  0.0307, -0.0210])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_hh_l0',\n",
       "               tensor([ 0.0204,  0.0298, -0.0173,  ..., -0.0159,  0.0120,  0.0150])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_ih_l0_reverse',\n",
       "               tensor([[-0.0092,  0.0295, -0.0237,  ..., -0.0166,  0.0269, -0.0093],\n",
       "                       [-0.0069,  0.0140, -0.0119,  ..., -0.0206, -0.0122,  0.0233],\n",
       "                       [-0.0247,  0.0262,  0.0209,  ...,  0.0071,  0.0118,  0.0049],\n",
       "                       ...,\n",
       "                       [ 0.0053,  0.0035,  0.0123,  ...,  0.0076,  0.0091, -0.0249],\n",
       "                       [-0.0288, -0.0048, -0.0271,  ...,  0.0261, -0.0058, -0.0186],\n",
       "                       [ 0.0079, -0.0160, -0.0190,  ...,  0.0076,  0.0140,  0.0081]])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_hh_l0_reverse',\n",
       "               tensor([[ 0.0179, -0.0043,  0.0096,  ...,  0.0021, -0.0116, -0.0148],\n",
       "                       [-0.0131, -0.0300,  0.0045,  ...,  0.0087, -0.0274,  0.0133],\n",
       "                       [-0.0244,  0.0266,  0.0197,  ..., -0.0002, -0.0273, -0.0085],\n",
       "                       ...,\n",
       "                       [-0.0250,  0.0042, -0.0174,  ...,  0.0301, -0.0297, -0.0162],\n",
       "                       [-0.0198,  0.0046,  0.0086,  ...,  0.0040,  0.0178,  0.0017],\n",
       "                       [-0.0300, -0.0014,  0.0006,  ..., -0.0160, -0.0124, -0.0309]])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_ih_l0_reverse',\n",
       "               tensor([-0.0160, -0.0153,  0.0094,  ..., -0.0284,  0.0310, -0.0290])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_hh_l0_reverse',\n",
       "               tensor([ 0.0032, -0.0079,  0.0038,  ..., -0.0111, -0.0213,  0.0146])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_ih_l1',\n",
       "               tensor([[ 0.0238,  0.0128, -0.0181,  ...,  0.0289,  0.0259,  0.0158],\n",
       "                       [-0.0079, -0.0136, -0.0109,  ..., -0.0029, -0.0209, -0.0072],\n",
       "                       [ 0.0091, -0.0278,  0.0030,  ...,  0.0244,  0.0044, -0.0123],\n",
       "                       ...,\n",
       "                       [ 0.0235, -0.0115,  0.0199,  ..., -0.0112, -0.0023,  0.0008],\n",
       "                       [ 0.0237, -0.0028,  0.0073,  ...,  0.0101, -0.0252,  0.0307],\n",
       "                       [-0.0097,  0.0309,  0.0209,  ...,  0.0285, -0.0124, -0.0212]])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_hh_l1',\n",
       "               tensor([[ 0.0099, -0.0128, -0.0103,  ..., -0.0140, -0.0033, -0.0023],\n",
       "                       [-0.0128, -0.0073, -0.0237,  ..., -0.0004, -0.0223,  0.0206],\n",
       "                       [ 0.0085,  0.0086,  0.0228,  ..., -0.0152,  0.0268,  0.0072],\n",
       "                       ...,\n",
       "                       [ 0.0018,  0.0029,  0.0233,  ...,  0.0274,  0.0101, -0.0290],\n",
       "                       [ 0.0306, -0.0053, -0.0255,  ..., -0.0044,  0.0229,  0.0105],\n",
       "                       [-0.0222,  0.0062,  0.0095,  ..., -0.0288, -0.0259, -0.0108]])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_ih_l1',\n",
       "               tensor([-0.0084,  0.0085,  0.0104,  ..., -0.0096, -0.0294,  0.0137])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_hh_l1',\n",
       "               tensor([-0.0140,  0.0095,  0.0267,  ..., -0.0151, -0.0308, -0.0123])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_ih_l1_reverse',\n",
       "               tensor([[ 3.4497e-03, -2.7880e-02,  1.2695e-02,  ..., -9.5178e-03,\n",
       "                         2.7542e-02, -1.3137e-02],\n",
       "                       [-2.6321e-02, -2.0438e-02,  8.6538e-06,  ...,  1.7928e-03,\n",
       "                        -4.9123e-04,  2.8490e-02],\n",
       "                       [ 9.6827e-04,  2.4260e-03,  2.6221e-02,  ..., -2.7656e-02,\n",
       "                        -1.0929e-02,  2.1191e-02],\n",
       "                       ...,\n",
       "                       [-1.8387e-02,  1.5855e-02,  1.4702e-02,  ..., -1.5533e-02,\n",
       "                        -4.1397e-03,  1.4542e-02],\n",
       "                       [ 1.4750e-02, -1.1295e-02,  1.2308e-02,  ...,  1.6463e-02,\n",
       "                         1.6390e-02, -1.4220e-02],\n",
       "                       [-2.5810e-02,  3.0005e-03, -1.4863e-02,  ...,  9.7539e-03,\n",
       "                        -1.2028e-02,  2.9900e-02]])),\n",
       "              ('nmt_decoder.nmt_decoder.weight_hh_l1_reverse',\n",
       "               tensor([[ 0.0284,  0.0122,  0.0098,  ..., -0.0074,  0.0003,  0.0238],\n",
       "                       [ 0.0025, -0.0207, -0.0284,  ...,  0.0242,  0.0079, -0.0051],\n",
       "                       [ 0.0276,  0.0104, -0.0264,  ...,  0.0296,  0.0309, -0.0299],\n",
       "                       ...,\n",
       "                       [ 0.0241, -0.0167,  0.0179,  ..., -0.0104,  0.0307,  0.0289],\n",
       "                       [ 0.0110,  0.0248,  0.0301,  ..., -0.0188,  0.0105,  0.0202],\n",
       "                       [ 0.0237,  0.0020, -0.0154,  ...,  0.0252,  0.0083,  0.0192]])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_ih_l1_reverse',\n",
       "               tensor([-0.0019, -0.0020,  0.0003,  ...,  0.0196, -0.0251, -0.0114])),\n",
       "              ('nmt_decoder.nmt_decoder.bias_hh_l1_reverse',\n",
       "               tensor([ 0.0125, -0.0191,  0.0307,  ..., -0.0065, -0.0086, -0.0076])),\n",
       "              ('nmt_decoder.fc.weight',\n",
       "               tensor([[ 0.0109,  0.0074,  0.0159,  ...,  0.0205,  0.0105, -0.0168],\n",
       "                       [ 0.0220,  0.0014, -0.0107,  ...,  0.0015, -0.0152,  0.0056],\n",
       "                       [-0.0013, -0.0182,  0.0070,  ...,  0.0064,  0.0161, -0.0035],\n",
       "                       ...,\n",
       "                       [ 0.0193,  0.0116,  0.0152,  ...,  0.0035, -0.0069, -0.0206],\n",
       "                       [ 0.0055,  0.0028,  0.0173,  ...,  0.0016,  0.0063, -0.0200],\n",
       "                       [ 0.0088, -0.0190,  0.0032,  ..., -0.0105,  0.0162,  0.0196]])),\n",
       "              ('nmt_decoder.fc.bias',\n",
       "               tensor([-0.0014, -0.0122, -0.0142,  ..., -0.0105, -0.0104, -0.0040])),\n",
       "              ('hidden2concat.weight',\n",
       "               tensor([[-0.0122,  0.0222,  0.0135,  ...,  0.0163, -0.0304,  0.0166],\n",
       "                       [ 0.0290, -0.0158, -0.0234,  ..., -0.0224,  0.0172, -0.0108],\n",
       "                       [ 0.0053, -0.0112,  0.0242,  ..., -0.0295, -0.0062, -0.0198],\n",
       "                       ...,\n",
       "                       [-0.0083,  0.0201, -0.0182,  ...,  0.0153, -0.0153, -0.0263],\n",
       "                       [ 0.0171,  0.0299, -0.0030,  ..., -0.0071,  0.0022, -0.0202],\n",
       "                       [-0.0239, -0.0006, -0.0032,  ...,  0.0122, -0.0282, -0.0311]])),\n",
       "              ('hidden2concat.bias',\n",
       "               tensor([ 0.0299, -0.0057,  0.0103,  0.0088,  0.0278, -0.0300,  0.0286, -0.0127,\n",
       "                        0.0293, -0.0261, -0.0263,  0.0035, -0.0237,  0.0234, -0.0198, -0.0236,\n",
       "                       -0.0129,  0.0270,  0.0117, -0.0177, -0.0246,  0.0103, -0.0275,  0.0223,\n",
       "                        0.0152, -0.0026, -0.0044,  0.0305,  0.0246,  0.0025, -0.0200, -0.0203,\n",
       "                        0.0006, -0.0155, -0.0183,  0.0098,  0.0310, -0.0063,  0.0309,  0.0111,\n",
       "                       -0.0210, -0.0267,  0.0159, -0.0302, -0.0069, -0.0246, -0.0027, -0.0117,\n",
       "                        0.0083,  0.0223,  0.0194, -0.0135,  0.0064,  0.0203, -0.0168, -0.0162,\n",
       "                        0.0292, -0.0246,  0.0185,  0.0008, -0.0183,  0.0252, -0.0264,  0.0029,\n",
       "                       -0.0187,  0.0243, -0.0069,  0.0153, -0.0188, -0.0007,  0.0101, -0.0205,\n",
       "                        0.0285, -0.0097,  0.0124, -0.0090, -0.0309, -0.0196,  0.0159,  0.0267,\n",
       "                        0.0126, -0.0247,  0.0209, -0.0220,  0.0035,  0.0283, -0.0297, -0.0229,\n",
       "                       -0.0048,  0.0219, -0.0243, -0.0293, -0.0125, -0.0226, -0.0068,  0.0167,\n",
       "                        0.0007,  0.0296, -0.0084, -0.0232, -0.0030,  0.0118,  0.0172, -0.0067,\n",
       "                       -0.0278,  0.0051, -0.0087,  0.0109, -0.0230,  0.0164,  0.0284,  0.0252,\n",
       "                        0.0020, -0.0141,  0.0192,  0.0086,  0.0146, -0.0244, -0.0161, -0.0007,\n",
       "                       -0.0223,  0.0132, -0.0255, -0.0070,  0.0295, -0.0218, -0.0304, -0.0113,\n",
       "                       -0.0184,  0.0291,  0.0047,  0.0195, -0.0163,  0.0139, -0.0069, -0.0233,\n",
       "                       -0.0190,  0.0294,  0.0307, -0.0268,  0.0291, -0.0028,  0.0198,  0.0050,\n",
       "                        0.0133, -0.0090,  0.0210,  0.0178,  0.0169, -0.0312,  0.0069, -0.0133,\n",
       "                        0.0039, -0.0184, -0.0170,  0.0060, -0.0074,  0.0103,  0.0180, -0.0181,\n",
       "                        0.0258, -0.0285, -0.0057,  0.0171, -0.0170,  0.0282,  0.0277,  0.0156,\n",
       "                       -0.0127,  0.0132, -0.0123, -0.0149, -0.0160,  0.0028, -0.0157, -0.0237,\n",
       "                       -0.0299,  0.0095, -0.0171, -0.0004, -0.0143, -0.0222, -0.0264, -0.0117,\n",
       "                        0.0216,  0.0260, -0.0107, -0.0050, -0.0126, -0.0230,  0.0215,  0.0017,\n",
       "                        0.0282, -0.0186, -0.0267,  0.0252,  0.0277,  0.0082,  0.0089,  0.0111,\n",
       "                        0.0205, -0.0014, -0.0177, -0.0309,  0.0270,  0.0155, -0.0106,  0.0171,\n",
       "                       -0.0288,  0.0165,  0.0005,  0.0286,  0.0141,  0.0073,  0.0038,  0.0190,\n",
       "                       -0.0007,  0.0088, -0.0059,  0.0272, -0.0239, -0.0097,  0.0087,  0.0129,\n",
       "                       -0.0262,  0.0035, -0.0278,  0.0242,  0.0306,  0.0207,  0.0049, -0.0130,\n",
       "                       -0.0186,  0.0168,  0.0074,  0.0134, -0.0238,  0.0155, -0.0212, -0.0019,\n",
       "                       -0.0206,  0.0055, -0.0082,  0.0049, -0.0129,  0.0270, -0.0126,  0.0226,\n",
       "                        0.0047, -0.0057, -0.0282, -0.0206, -0.0209, -0.0281, -0.0227, -0.0193,\n",
       "                        0.0091,  0.0235, -0.0120,  0.0043,  0.0234, -0.0219, -0.0257,  0.0082,\n",
       "                       -0.0131,  0.0236,  0.0091, -0.0100, -0.0126,  0.0068,  0.0006, -0.0129,\n",
       "                       -0.0151, -0.0083, -0.0179, -0.0123, -0.0240, -0.0039,  0.0072,  0.0170,\n",
       "                        0.0211,  0.0059, -0.0244, -0.0236, -0.0304,  0.0222, -0.0284, -0.0108,\n",
       "                        0.0228,  0.0306,  0.0112, -0.0007, -0.0287, -0.0246,  0.0281, -0.0150,\n",
       "                        0.0010,  0.0312, -0.0108,  0.0267, -0.0178, -0.0235,  0.0209,  0.0211,\n",
       "                        0.0020, -0.0065,  0.0248,  0.0220,  0.0093, -0.0190, -0.0046, -0.0120,\n",
       "                       -0.0077,  0.0310, -0.0275, -0.0088, -0.0043, -0.0056,  0.0212, -0.0070,\n",
       "                       -0.0243, -0.0183,  0.0180,  0.0308, -0.0113,  0.0024, -0.0041, -0.0140,\n",
       "                        0.0256, -0.0040,  0.0030, -0.0288,  0.0211, -0.0209,  0.0040, -0.0274,\n",
       "                       -0.0273,  0.0010,  0.0045, -0.0310,  0.0149, -0.0249,  0.0028,  0.0038,\n",
       "                       -0.0048,  0.0150,  0.0191, -0.0156, -0.0134, -0.0082,  0.0070, -0.0248,\n",
       "                        0.0215,  0.0088, -0.0190,  0.0139, -0.0226, -0.0233, -0.0001, -0.0071,\n",
       "                       -0.0163, -0.0022, -0.0105,  0.0204,  0.0279,  0.0113,  0.0129, -0.0073,\n",
       "                       -0.0118,  0.0295,  0.0302,  0.0108, -0.0189,  0.0168, -0.0065,  0.0163,\n",
       "                       -0.0152, -0.0159, -0.0255, -0.0208,  0.0192, -0.0037, -0.0194,  0.0046,\n",
       "                       -0.0130,  0.0116,  0.0248,  0.0158, -0.0176, -0.0237,  0.0242,  0.0005,\n",
       "                        0.0262, -0.0208, -0.0087, -0.0207, -0.0253,  0.0056, -0.0118,  0.0223,\n",
       "                       -0.0041, -0.0071,  0.0198,  0.0053, -0.0252,  0.0060, -0.0311, -0.0156,\n",
       "                        0.0045,  0.0051,  0.0201, -0.0248,  0.0198,  0.0103,  0.0233, -0.0034,\n",
       "                       -0.0242,  0.0197,  0.0204,  0.0307, -0.0046,  0.0191,  0.0043, -0.0238,\n",
       "                       -0.0248,  0.0045, -0.0201, -0.0210, -0.0266, -0.0031,  0.0217,  0.0275,\n",
       "                        0.0156, -0.0040,  0.0244, -0.0227,  0.0039, -0.0185, -0.0280,  0.0234,\n",
       "                       -0.0197,  0.0088, -0.0013, -0.0141,  0.0138,  0.0008,  0.0165, -0.0068,\n",
       "                        0.0276,  0.0070, -0.0238,  0.0042,  0.0277, -0.0082, -0.0095, -0.0228,\n",
       "                        0.0184, -0.0004, -0.0267, -0.0219, -0.0163,  0.0249, -0.0145,  0.0114,\n",
       "                       -0.0294, -0.0301, -0.0224, -0.0081,  0.0247, -0.0035, -0.0194, -0.0239,\n",
       "                       -0.0132, -0.0013, -0.0001, -0.0149,  0.0260, -0.0227,  0.0280,  0.0190,\n",
       "                       -0.0262,  0.0009,  0.0249,  0.0213,  0.0097, -0.0076,  0.0067,  0.0122,\n",
       "                       -0.0241, -0.0029,  0.0046, -0.0041,  0.0277, -0.0114,  0.0035, -0.0128,\n",
       "                       -0.0132, -0.0163,  0.0172,  0.0050,  0.0035,  0.0173, -0.0037,  0.0208,\n",
       "                       -0.0232,  0.0215, -0.0130,  0.0268,  0.0064, -0.0090, -0.0284, -0.0273])),\n",
       "              ('latent2concat.weight',\n",
       "               tensor([[ 2.3495e-02, -2.6457e-03,  2.9414e-02,  ..., -1.6697e-02,\n",
       "                        -3.1007e-02,  2.0913e-02],\n",
       "                       [ 2.2094e-02,  5.1569e-03, -5.7697e-03,  ...,  2.1035e-02,\n",
       "                        -2.9727e-02,  2.1619e-02],\n",
       "                       [ 1.0127e-02,  1.4652e-02,  1.8158e-02,  ..., -2.3703e-03,\n",
       "                         3.2401e-03, -1.6499e-02],\n",
       "                       ...,\n",
       "                       [ 2.7555e-03, -1.6822e-02, -1.8868e-02,  ...,  2.8986e-02,\n",
       "                         1.9100e-02, -1.7222e-02],\n",
       "                       [ 2.6319e-02,  2.5844e-02, -2.9724e-02,  ..., -2.0474e-02,\n",
       "                        -3.8642e-05, -6.4869e-03],\n",
       "                       [-8.0423e-03,  8.6783e-03, -5.8276e-03,  ...,  4.1137e-03,\n",
       "                         1.2003e-02,  1.3902e-02]])),\n",
       "              ('latent2concat.bias',\n",
       "               tensor([-1.8047e-02, -2.2508e-02,  6.0331e-03, -5.5631e-03, -3.8822e-03,\n",
       "                        2.9841e-02, -1.0659e-02,  2.2052e-02,  8.8289e-04,  2.2507e-03,\n",
       "                       -1.1534e-03,  1.4280e-02, -2.3919e-02, -2.7992e-02,  1.8012e-02,\n",
       "                        6.8009e-03, -5.8873e-03, -7.7513e-03,  7.2962e-03,  3.2791e-04,\n",
       "                       -1.4267e-02, -1.5569e-02,  2.5943e-02,  1.8043e-02, -5.4261e-03,\n",
       "                        3.0569e-02, -1.3844e-02,  2.0663e-02,  7.6488e-03, -2.7263e-02,\n",
       "                       -7.5602e-03,  3.0046e-02, -1.0041e-02, -1.1453e-02,  2.2351e-02,\n",
       "                        2.1173e-02, -4.6594e-03,  3.4512e-04, -2.1845e-02,  1.0178e-03,\n",
       "                       -5.1429e-03,  2.1464e-02, -1.4740e-02,  2.5640e-02, -2.1978e-02,\n",
       "                        3.1069e-02, -9.7980e-03,  1.3600e-02,  1.7458e-02, -2.4758e-02,\n",
       "                        5.5217e-03, -3.0332e-02,  1.9098e-04,  1.4167e-02, -5.7663e-03,\n",
       "                       -2.3204e-02, -8.7268e-03, -1.0948e-02, -2.0302e-02,  9.6329e-03,\n",
       "                       -2.2557e-02,  1.6298e-02, -4.5866e-04,  2.2168e-02, -2.7964e-02,\n",
       "                        1.3288e-02, -6.0589e-03,  7.4684e-03,  2.1194e-02,  1.6897e-02,\n",
       "                        2.8680e-02, -6.7427e-03, -4.2304e-03, -1.9442e-02, -6.3210e-03,\n",
       "                       -8.4382e-03, -1.4799e-02, -2.6377e-03,  8.3369e-03, -3.3315e-03,\n",
       "                        1.0312e-02,  7.8472e-03,  1.9936e-02,  2.9614e-02,  1.7744e-02,\n",
       "                       -8.4612e-03,  2.2422e-02,  8.3663e-03, -1.1919e-02,  7.7299e-03,\n",
       "                        4.6093e-03,  2.0653e-02,  2.8138e-02, -8.8498e-03,  9.1698e-04,\n",
       "                        9.3401e-03, -3.6290e-03, -1.0467e-02,  7.7913e-03,  2.8156e-02,\n",
       "                        2.1311e-02,  3.0278e-02, -8.2136e-03,  3.0442e-02,  1.9550e-02,\n",
       "                       -9.5209e-03,  2.5701e-02,  2.1715e-02,  2.3780e-02,  8.1675e-03,\n",
       "                       -1.6331e-02, -2.9432e-02, -2.8827e-02, -1.8826e-02, -7.5281e-03,\n",
       "                        1.9507e-02, -1.2024e-02,  1.5923e-02,  2.7578e-02, -1.1284e-02,\n",
       "                        1.2727e-02,  2.7707e-03, -1.2837e-02, -2.0621e-02,  1.6424e-02,\n",
       "                        2.2249e-02, -1.8106e-02, -1.9166e-02,  1.8221e-02, -1.6011e-02,\n",
       "                        2.9328e-02, -1.3456e-02, -2.3358e-02, -5.4865e-03, -2.9440e-02,\n",
       "                        1.1850e-02, -7.5110e-03, -3.1035e-02, -1.1775e-02,  1.6729e-02,\n",
       "                        1.0301e-02, -3.1179e-02, -1.2349e-02,  1.2751e-02, -1.7066e-02,\n",
       "                       -1.8862e-02,  1.4123e-02, -1.4475e-02, -2.7516e-02, -2.8720e-02,\n",
       "                        7.9554e-03,  2.5226e-02,  1.8160e-02,  4.0271e-03,  2.1462e-02,\n",
       "                        2.8361e-02,  1.4500e-02,  2.2904e-02,  1.2474e-02, -2.6856e-02,\n",
       "                       -1.2952e-02,  3.0213e-02,  2.0253e-02,  1.7394e-02,  7.6603e-03,\n",
       "                       -1.0697e-02, -8.0927e-03, -7.3085e-03,  1.5312e-02, -1.5499e-02,\n",
       "                       -1.7753e-02,  2.2077e-02,  2.8760e-02,  2.2289e-02, -2.0036e-02,\n",
       "                        3.1406e-04, -2.4966e-02, -2.4942e-02,  2.3003e-02, -1.7157e-02,\n",
       "                       -6.5775e-03,  2.1348e-02,  3.3862e-03,  6.9084e-03,  1.1084e-02,\n",
       "                       -4.9814e-03, -1.9517e-02, -1.2732e-02, -1.7399e-02, -1.7572e-02,\n",
       "                        2.6815e-02,  2.5823e-02,  1.6051e-02, -1.5380e-03,  3.0133e-02,\n",
       "                        3.0793e-02,  1.9694e-02, -1.0967e-02, -2.7574e-02, -5.8304e-04,\n",
       "                        1.2674e-02, -1.7584e-02, -5.9858e-03, -1.2079e-02, -1.9109e-03,\n",
       "                       -2.0160e-02, -7.5554e-03,  2.3003e-02, -8.2667e-03,  2.6315e-02,\n",
       "                       -6.5004e-03, -1.5432e-03, -2.7875e-02,  1.9328e-02, -2.2742e-02,\n",
       "                        5.8058e-03,  9.9863e-03, -3.1186e-02,  2.9394e-02,  3.0427e-02,\n",
       "                       -9.8176e-03, -2.1421e-02,  1.6959e-02, -1.5929e-02, -3.0146e-02,\n",
       "                        1.2985e-02, -4.0683e-03,  2.7300e-03,  2.5831e-02, -4.1903e-03,\n",
       "                       -2.9672e-02,  1.1003e-02,  2.9445e-02, -2.3071e-02, -3.7611e-03,\n",
       "                        2.8203e-02,  1.5697e-02,  1.6287e-02, -1.0640e-03,  1.3883e-02,\n",
       "                       -3.8537e-03,  1.5649e-02,  2.2848e-02,  2.5648e-02, -1.3890e-02,\n",
       "                        1.7106e-02, -2.8248e-02, -1.9429e-02,  2.4130e-03, -3.8101e-04,\n",
       "                       -1.4443e-02,  3.0152e-02, -7.1937e-03, -1.5631e-02, -4.8697e-03,\n",
       "                        1.8077e-02, -2.4157e-02,  1.5219e-02, -5.2110e-03,  1.3175e-02,\n",
       "                       -1.3542e-02,  1.7808e-02,  1.4147e-02, -3.7537e-04,  2.2716e-02,\n",
       "                        6.4775e-03, -8.8260e-03,  4.0884e-03, -2.3913e-02,  3.0826e-02,\n",
       "                        2.6751e-03, -5.5313e-03, -1.9800e-02, -6.8900e-03, -2.4007e-02,\n",
       "                       -1.0934e-02,  9.3928e-03, -5.2213e-03,  1.1908e-02, -1.9241e-02,\n",
       "                        2.8998e-03,  3.2118e-03,  1.2877e-02, -2.4444e-02, -9.6237e-03,\n",
       "                        6.4991e-03, -2.3104e-02,  1.2084e-02, -2.7759e-02,  1.9829e-02,\n",
       "                       -1.3032e-02,  2.2361e-02, -1.7821e-02,  2.7079e-02, -1.0287e-02,\n",
       "                       -2.9261e-02, -8.8629e-03, -1.9542e-03, -1.6456e-02,  1.2699e-02,\n",
       "                       -9.9286e-03,  1.4741e-02, -2.5650e-02,  2.1230e-02,  4.5463e-03,\n",
       "                        2.7323e-02,  2.2917e-02, -2.3538e-02, -1.3098e-02, -2.3516e-02,\n",
       "                       -1.5084e-02,  1.7057e-02, -8.9634e-03,  7.4724e-03,  1.1266e-02,\n",
       "                        1.3972e-03, -3.6913e-03,  2.3962e-04, -1.5277e-02, -1.6731e-02,\n",
       "                       -2.3964e-02,  1.3753e-02,  1.4505e-02, -1.7768e-02,  1.9516e-02,\n",
       "                        1.0756e-02,  1.0807e-02,  1.4166e-02, -2.0015e-03,  7.7968e-03,\n",
       "                        1.4227e-02, -9.9813e-03, -2.2028e-02,  1.9752e-04, -3.0441e-02,\n",
       "                        1.2741e-02, -5.0308e-03,  2.5207e-02, -2.6975e-02, -2.8287e-02,\n",
       "                       -1.0566e-02, -2.1234e-02,  2.3269e-02,  2.9625e-02, -1.9783e-02,\n",
       "                       -1.2528e-02, -1.2549e-02, -7.6998e-03,  2.8562e-02, -2.6002e-02,\n",
       "                       -2.0507e-02,  2.0636e-02, -7.8449e-03,  9.8270e-03, -4.0930e-03,\n",
       "                        7.3476e-03, -2.3893e-02, -8.5509e-03,  1.5038e-02, -3.5450e-03,\n",
       "                       -1.2723e-02,  3.0953e-02,  3.0169e-02,  1.7541e-02,  2.3835e-02,\n",
       "                        2.2195e-02, -2.4896e-02,  2.8174e-02,  2.8375e-02,  5.7475e-03,\n",
       "                        1.3755e-02, -7.9819e-03,  3.1111e-02, -4.4668e-03,  2.7477e-02,\n",
       "                        2.7496e-02,  1.5881e-02,  2.0574e-02, -2.4221e-02, -1.4157e-02,\n",
       "                        2.1406e-02,  2.8959e-02,  1.9729e-02, -3.0957e-02,  1.2706e-02,\n",
       "                       -2.8098e-02,  2.6130e-02,  2.8795e-02,  2.8816e-02, -2.3169e-02,\n",
       "                       -1.4181e-02, -1.9073e-02,  1.3850e-02, -7.3847e-03,  1.3878e-02,\n",
       "                       -1.8431e-02, -2.8754e-02,  1.0365e-02,  2.5414e-02, -3.0088e-02,\n",
       "                       -2.4690e-02,  1.4804e-02, -6.6734e-03,  2.6084e-02,  1.3617e-02,\n",
       "                        2.5126e-02,  6.4986e-03, -4.9468e-03, -8.0696e-03, -1.7474e-03,\n",
       "                        1.5660e-02, -4.5835e-03, -7.3919e-03,  2.5773e-03,  1.0850e-02,\n",
       "                       -2.9785e-02,  2.5133e-02, -7.3824e-05,  2.0949e-02,  9.6025e-03,\n",
       "                        1.0316e-02,  1.7537e-03,  1.5716e-03, -1.1782e-02, -6.3080e-03,\n",
       "                        2.4323e-02, -8.9966e-05,  2.7572e-02,  4.3511e-03,  1.4866e-02,\n",
       "                       -3.0628e-02, -1.9910e-02,  2.2112e-02, -6.9374e-03,  2.1635e-02,\n",
       "                        2.9863e-02, -2.1985e-02, -2.0865e-02,  2.6553e-02, -9.1930e-03,\n",
       "                       -2.1020e-02,  1.0137e-03,  6.4899e-03, -2.5728e-02, -2.6289e-03,\n",
       "                        1.9792e-02, -7.2611e-03,  2.4221e-02, -2.4969e-02, -5.3911e-04,\n",
       "                       -1.4385e-02, -7.0152e-03,  9.8997e-03,  3.0970e-02,  7.2911e-03,\n",
       "                        1.4403e-02,  5.2591e-03, -1.9908e-02,  2.6957e-02,  1.9724e-03,\n",
       "                       -2.0688e-02,  1.9966e-02, -6.5158e-03,  9.5165e-03,  1.7939e-02,\n",
       "                        2.0315e-03, -2.6304e-02,  2.6348e-02,  2.2272e-02,  1.6288e-03,\n",
       "                       -1.9181e-02, -8.3186e-03, -3.0121e-02, -4.2985e-03, -1.3608e-02,\n",
       "                        2.0317e-02, -2.6046e-02,  1.4476e-02, -1.3329e-02,  1.3405e-02,\n",
       "                       -6.1664e-03, -4.3061e-03, -2.7783e-02, -2.6888e-03, -1.5854e-02,\n",
       "                        6.0068e-03,  2.7753e-02,  4.7160e-03, -2.1414e-02,  9.1377e-04,\n",
       "                       -1.9024e-02, -2.5633e-02, -2.3756e-02, -2.5582e-02,  1.1269e-03,\n",
       "                        1.4992e-02, -1.2142e-02, -1.3651e-02, -1.8608e-02, -1.0459e-03,\n",
       "                        6.2530e-03,  2.7817e-02,  1.3145e-02,  9.1148e-03, -3.0090e-02,\n",
       "                        1.2245e-02,  1.2724e-02, -2.1278e-02, -1.3988e-02, -2.7982e-04,\n",
       "                        1.5834e-02,  7.0433e-03]))])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_encoder = Encoder()\n",
    "nmt_model = StylizedNMT(nmt_encoder, nmt_decoder, d_hidden, total_latent, device)\n",
    "torch.load(\"../../kcc_data/nmt_model.pth\", map_location=torch.device('cpu'))\n",
    "nmt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': OrderedDict([('encoder.src_embedding.weight',\n",
       "               tensor([[ 0.2827,  0.0358,  0.5101,  ...,  0.7153, -0.3080, -1.2164],\n",
       "                       [-1.5855, -0.7134, -1.4938,  ..., -1.3202, -0.1101,  1.6480],\n",
       "                       [-0.0306,  0.3408,  0.4493,  ..., -0.8764,  0.0742,  0.0642],\n",
       "                       ...,\n",
       "                       [-1.4204,  0.5075, -0.1172,  ..., -0.3030,  1.2276,  0.1671],\n",
       "                       [-0.7858, -0.6558,  0.7487,  ..., -2.0158, -0.4633,  0.5521],\n",
       "                       [-1.1546, -0.8147, -0.0947,  ..., -0.2383, -1.4264,  0.5645]])),\n",
       "              ('encoder.encoder.weight_ih_l0',\n",
       "               tensor([[-0.0419, -0.0469, -0.0271,  ...,  0.0011,  0.0503,  0.0204],\n",
       "                       [ 0.0112, -0.0291,  0.0083,  ...,  0.0113, -0.0264,  0.0207],\n",
       "                       [ 0.0178,  0.0250, -0.0021,  ...,  0.0339, -0.0029,  0.0152],\n",
       "                       ...,\n",
       "                       [ 0.0917,  0.0614,  0.0323,  ...,  0.0208,  0.0350, -0.0025],\n",
       "                       [ 0.0216,  0.0163,  0.0107,  ...,  0.0154,  0.0132, -0.0065],\n",
       "                       [-0.0724,  0.0118,  0.0004,  ...,  0.0278,  0.0185, -0.0117]])),\n",
       "              ('encoder.encoder.weight_hh_l0',\n",
       "               tensor([[-0.0136,  0.0130, -0.0144,  ..., -0.0111,  0.0213, -0.0128],\n",
       "                       [-0.0235,  0.0193, -0.0309,  ..., -0.0232, -0.0244, -0.0429],\n",
       "                       [ 0.0311,  0.0109, -0.0153,  ...,  0.0136, -0.0451,  0.0405],\n",
       "                       ...,\n",
       "                       [ 0.0096,  0.0234, -0.0037,  ..., -0.0292, -0.0086,  0.0056],\n",
       "                       [-0.0043,  0.0109, -0.0248,  ..., -0.0181,  0.0109, -0.0210],\n",
       "                       [ 0.0335, -0.0037,  0.0222,  ...,  0.0228,  0.0136,  0.0477]])),\n",
       "              ('encoder.encoder.bias_ih_l0',\n",
       "               tensor([-0.0550, -0.0492, -0.0223,  ..., -0.0142, -0.0089, -0.0228])),\n",
       "              ('encoder.encoder.bias_hh_l0',\n",
       "               tensor([-0.0384, -0.0368, -0.0314,  ...,  0.0096, -0.0099, -0.0107])),\n",
       "              ('encoder.encoder.weight_ih_l0_reverse',\n",
       "               tensor([[ 0.0120,  0.0361, -0.0190,  ...,  0.0340, -0.0059, -0.0071],\n",
       "                       [ 0.0428, -0.0168, -0.0980,  ...,  0.0360, -0.0401,  0.0417],\n",
       "                       [ 0.0407, -0.0020, -0.0708,  ...,  0.0731,  0.0614, -0.0541],\n",
       "                       ...,\n",
       "                       [-0.0021, -0.0155,  0.0157,  ...,  0.0239,  0.0060, -0.0180],\n",
       "                       [ 0.0099,  0.0207,  0.0220,  ...,  0.0216,  0.0375,  0.0028],\n",
       "                       [-0.0059,  0.0089,  0.0298,  ...,  0.0094, -0.0140, -0.0185]])),\n",
       "              ('encoder.encoder.weight_hh_l0_reverse',\n",
       "               tensor([[ 0.0340, -0.0462, -0.0178,  ..., -0.0139,  0.0316,  0.0019],\n",
       "                       [-0.0232,  0.0305, -0.0344,  ...,  0.0777,  0.0557,  0.0466],\n",
       "                       [-0.0581, -0.0437,  0.0190,  ..., -0.0198, -0.0479, -0.0459],\n",
       "                       ...,\n",
       "                       [-0.0331, -0.0147,  0.0155,  ..., -0.0384, -0.0338,  0.0054],\n",
       "                       [-0.0213,  0.0354, -0.0163,  ...,  0.0186, -0.0515,  0.0220],\n",
       "                       [ 0.0006, -0.0068, -0.0206,  ..., -0.0617,  0.0005, -0.0269]])),\n",
       "              ('encoder.encoder.bias_ih_l0_reverse',\n",
       "               tensor([-0.0344, -0.0359, -0.0461,  ..., -0.0155, -0.0374,  0.0087])),\n",
       "              ('encoder.encoder.bias_hh_l0_reverse',\n",
       "               tensor([-0.0824, -0.0082, -0.0265,  ...,  0.0048,  0.0043,  0.0056])),\n",
       "              ('encoder.encoder.weight_ih_l1',\n",
       "               tensor([[-0.0158, -0.0168, -0.0018,  ...,  0.0085, -0.0079,  0.0145],\n",
       "                       [-0.0019, -0.0099,  0.0287,  ..., -0.0294,  0.0203,  0.0207],\n",
       "                       [ 0.0111,  0.0368,  0.0205,  ..., -0.0150, -0.0243,  0.0167],\n",
       "                       ...,\n",
       "                       [-0.0305, -0.0158, -0.0236,  ..., -0.0098, -0.0031,  0.0042],\n",
       "                       [ 0.0185,  0.0157, -0.0082,  ...,  0.0316,  0.0257, -0.0099],\n",
       "                       [-0.0263,  0.0256,  0.0068,  ..., -0.0071,  0.0301, -0.0402]])),\n",
       "              ('encoder.encoder.weight_hh_l1',\n",
       "               tensor([[-0.0213, -0.0171, -0.0031,  ...,  0.0357, -0.0159, -0.0019],\n",
       "                       [-0.0026, -0.0063, -0.0099,  ...,  0.0326, -0.0126,  0.0196],\n",
       "                       [ 0.0202,  0.0348, -0.0201,  ..., -0.0258,  0.0343, -0.0024],\n",
       "                       ...,\n",
       "                       [ 0.0269, -0.0027, -0.0020,  ...,  0.0209, -0.0083,  0.0041],\n",
       "                       [ 0.0163, -0.0080,  0.0072,  ..., -0.0270, -0.0080, -0.0142],\n",
       "                       [-0.0091, -0.0198, -0.0033,  ...,  0.0165,  0.0215, -0.0087]])),\n",
       "              ('encoder.encoder.bias_ih_l1',\n",
       "               tensor([-0.0332, -0.0279, -0.0335,  ...,  0.0176,  0.0282,  0.0135])),\n",
       "              ('encoder.encoder.bias_hh_l1',\n",
       "               tensor([ 0.0084,  0.0038,  0.0159,  ..., -0.0316,  0.0230,  0.0061])),\n",
       "              ('encoder.encoder.weight_ih_l1_reverse',\n",
       "               tensor([[ 0.0884,  0.0669, -0.0036,  ..., -0.0132,  0.0132, -0.0247],\n",
       "                       [ 0.0525, -0.0040,  0.0121,  ..., -0.0300,  0.0306, -0.0292],\n",
       "                       [ 0.0207,  0.0148,  0.0453,  ..., -0.0218, -0.0055, -0.0339],\n",
       "                       ...,\n",
       "                       [-0.0214, -0.0473,  0.0309,  ..., -0.0300,  0.0033, -0.0348],\n",
       "                       [-0.0662, -0.0537,  0.0654,  ..., -0.0090, -0.0028,  0.0030],\n",
       "                       [-0.0499, -0.0530,  0.0104,  ..., -0.0035,  0.0018, -0.0090]])),\n",
       "              ('encoder.encoder.weight_hh_l1_reverse',\n",
       "               tensor([[-0.0105, -0.0056,  0.0168,  ..., -0.0225,  0.0179, -0.0064],\n",
       "                       [-0.0278,  0.0441, -0.0769,  ...,  0.0023, -0.0019,  0.0139],\n",
       "                       [-0.0300,  0.0285, -0.0105,  ...,  0.0216, -0.0046, -0.0097],\n",
       "                       ...,\n",
       "                       [-0.0173, -0.0342, -0.0260,  ..., -0.0209, -0.0124, -0.0278],\n",
       "                       [-0.0253,  0.0017, -0.0336,  ...,  0.0054, -0.0505, -0.0377],\n",
       "                       [-0.0132, -0.0025, -0.0100,  ...,  0.0275,  0.0086, -0.0187]])),\n",
       "              ('encoder.encoder.bias_ih_l1_reverse',\n",
       "               tensor([ 0.0143, -0.0287,  0.0167,  ...,  0.0054,  0.0008, -0.0420])),\n",
       "              ('encoder.encoder.bias_hh_l1_reverse',\n",
       "               tensor([ 0.0014, -0.0209,  0.0077,  ...,  0.0018, -0.0097, -0.0246])),\n",
       "              ('tst_decoder.trg_embedding.weight',\n",
       "               tensor([[-1.6109,  0.3463, -3.5884,  ..., -2.1805,  1.3806, -0.1780],\n",
       "                       [ 0.4065, -1.3162,  0.5481,  ..., -0.6845, -0.1981, -0.4816],\n",
       "                       [-0.7008,  1.6261, -0.9445,  ...,  1.4529,  0.8211, -1.3521],\n",
       "                       ...,\n",
       "                       [-0.7911, -0.4229, -1.2373,  ..., -2.1452,  0.4733,  0.3252],\n",
       "                       [-0.7931,  0.5045,  0.4473,  ..., -0.5171, -0.7589,  0.0882],\n",
       "                       [ 0.9742,  0.1630,  1.8031,  ...,  0.3974,  0.3039, -0.4928]])),\n",
       "              ('tst_decoder.tst_decoder.weight_ih_l0',\n",
       "               tensor([[ 5.9253e-02, -5.5323e-03, -3.4965e-02,  ..., -1.6802e-02,\n",
       "                         5.8636e-03, -2.4294e-02],\n",
       "                       [-2.9425e-02, -4.4062e-02, -6.3525e-05,  ..., -2.3356e-02,\n",
       "                        -7.1058e-02,  2.0072e-02],\n",
       "                       [ 2.4570e-03, -2.3199e-02, -2.0466e-02,  ...,  3.8958e-02,\n",
       "                         1.6118e-02,  4.0375e-03],\n",
       "                       ...,\n",
       "                       [ 2.5403e-02, -1.7122e-02,  3.2306e-02,  ...,  4.1354e-02,\n",
       "                        -1.4932e-02, -3.4986e-03],\n",
       "                       [ 3.3775e-02,  2.3249e-02,  3.5758e-02,  ...,  1.4475e-02,\n",
       "                         3.4245e-02, -1.1845e-03],\n",
       "                       [ 5.0402e-04, -5.9682e-03,  2.1196e-02,  ..., -8.9503e-03,\n",
       "                        -2.9112e-02,  4.2075e-02]])),\n",
       "              ('tst_decoder.tst_decoder.weight_hh_l0',\n",
       "               tensor([[ 0.0152, -0.0179,  0.0168,  ..., -0.0188, -0.0059, -0.0193],\n",
       "                       [-0.0060, -0.0160,  0.0081,  ..., -0.0664, -0.0339,  0.0106],\n",
       "                       [ 0.0023,  0.0509,  0.0114,  ..., -0.0387, -0.0115, -0.0105],\n",
       "                       ...,\n",
       "                       [-0.0105, -0.0037, -0.0053,  ...,  0.0077, -0.0104,  0.0010],\n",
       "                       [-0.0126,  0.0141, -0.0238,  ..., -0.0408, -0.0078,  0.0151],\n",
       "                       [ 0.0157, -0.0039,  0.0381,  ..., -0.0218, -0.0198,  0.0271]])),\n",
       "              ('tst_decoder.tst_decoder.bias_ih_l0',\n",
       "               tensor([ 0.0257,  0.0213,  0.0123,  ...,  0.0011,  0.0204, -0.0222])),\n",
       "              ('tst_decoder.tst_decoder.bias_hh_l0',\n",
       "               tensor([ 0.0364,  0.0183,  0.0259,  ...,  0.0535, -0.0261, -0.0061])),\n",
       "              ('tst_decoder.tst_decoder.weight_ih_l0_reverse',\n",
       "               tensor([[-0.0076, -0.0285,  0.0136,  ...,  0.0083, -0.0379,  0.0080],\n",
       "                       [ 0.0409, -0.0261,  0.0156,  ..., -0.0474, -0.0176, -0.0065],\n",
       "                       [-0.0373, -0.0275, -0.0407,  ..., -0.0133,  0.0539, -0.0217],\n",
       "                       ...,\n",
       "                       [ 0.0100, -0.0182, -0.0470,  ..., -0.0315,  0.0064,  0.0383],\n",
       "                       [ 0.0012,  0.0074, -0.0303,  ..., -0.0177, -0.0142, -0.0075],\n",
       "                       [ 0.0136, -0.0299, -0.0425,  ..., -0.0272, -0.0400,  0.0383]])),\n",
       "              ('tst_decoder.tst_decoder.weight_hh_l0_reverse',\n",
       "               tensor([[ 0.0293,  0.0275,  0.0240,  ...,  0.0224, -0.0227, -0.0117],\n",
       "                       [ 0.0197, -0.0270, -0.0479,  ..., -0.0497, -0.0206,  0.0319],\n",
       "                       [ 0.0350, -0.0200,  0.0215,  ...,  0.0103,  0.0063,  0.0645],\n",
       "                       ...,\n",
       "                       [ 0.0030, -0.0004, -0.0543,  ...,  0.0412, -0.0067, -0.0373],\n",
       "                       [ 0.0121,  0.0243, -0.0056,  ...,  0.0010,  0.0014,  0.0052],\n",
       "                       [ 0.0324, -0.0314, -0.0522,  ...,  0.0018,  0.0118,  0.0281]])),\n",
       "              ('tst_decoder.tst_decoder.bias_ih_l0_reverse',\n",
       "               tensor([ 0.0016, -0.0025,  0.0498,  ...,  0.0653, -0.0212,  0.0045])),\n",
       "              ('tst_decoder.tst_decoder.bias_hh_l0_reverse',\n",
       "               tensor([ 0.0213, -0.0138,  0.0640,  ...,  0.0469, -0.0061,  0.0063])),\n",
       "              ('tst_decoder.tst_decoder.weight_ih_l1',\n",
       "               tensor([[ 0.0256,  0.0319,  0.0033,  ...,  0.0109, -0.0188, -0.0309],\n",
       "                       [-0.0044, -0.0067,  0.0237,  ...,  0.0787,  0.0171, -0.0448],\n",
       "                       [ 0.0129, -0.0136,  0.0113,  ..., -0.0141, -0.0091,  0.0489],\n",
       "                       ...,\n",
       "                       [-0.0212,  0.0113, -0.0232,  ..., -0.0763, -0.0242,  0.0123],\n",
       "                       [ 0.0146, -0.0067, -0.0386,  ..., -0.0012, -0.0053, -0.0290],\n",
       "                       [ 0.0009, -0.0240, -0.0216,  ..., -0.0146,  0.0262, -0.0407]])),\n",
       "              ('tst_decoder.tst_decoder.weight_hh_l1',\n",
       "               tensor([[-0.0210, -0.0259,  0.0163,  ...,  0.0114, -0.0065, -0.0128],\n",
       "                       [ 0.0065,  0.0285, -0.0233,  ..., -0.0248,  0.0196, -0.0251],\n",
       "                       [ 0.0156,  0.0005, -0.0260,  ..., -0.0084, -0.0152,  0.0229],\n",
       "                       ...,\n",
       "                       [-0.0081,  0.0241, -0.0348,  ...,  0.0094,  0.0221, -0.0163],\n",
       "                       [ 0.0156,  0.0136, -0.0074,  ..., -0.0274, -0.0165, -0.0440],\n",
       "                       [ 0.0024, -0.0176,  0.0055,  ..., -0.0269, -0.0097, -0.0106]])),\n",
       "              ('tst_decoder.tst_decoder.bias_ih_l1',\n",
       "               tensor([-0.0204, -0.0170,  0.0092,  ...,  0.0118, -0.0421, -0.0288])),\n",
       "              ('tst_decoder.tst_decoder.bias_hh_l1',\n",
       "               tensor([ 0.0042,  0.0210,  0.0041,  ..., -0.0038,  0.0167,  0.0002])),\n",
       "              ('tst_decoder.tst_decoder.weight_ih_l1_reverse',\n",
       "               tensor([[-0.0495,  0.0457, -0.0203,  ..., -0.0270,  0.0056,  0.0302],\n",
       "                       [-0.0224,  0.0071, -0.0335,  ...,  0.0273, -0.0070,  0.0373],\n",
       "                       [ 0.0249, -0.0137, -0.0096,  ...,  0.0022,  0.0086,  0.0113],\n",
       "                       ...,\n",
       "                       [-0.0221,  0.0006, -0.0010,  ..., -0.0336,  0.0227, -0.0328],\n",
       "                       [ 0.0063, -0.0198, -0.0228,  ..., -0.0260,  0.0138,  0.0375],\n",
       "                       [-0.0152, -0.0029,  0.0037,  ...,  0.0787,  0.0081,  0.0070]])),\n",
       "              ('tst_decoder.tst_decoder.weight_hh_l1_reverse',\n",
       "               tensor([[ 0.0263,  0.0524,  0.0131,  ...,  0.0051,  0.0075,  0.0022],\n",
       "                       [ 0.0337,  0.0080,  0.0263,  ...,  0.0464, -0.0002,  0.0232],\n",
       "                       [ 0.0025,  0.0467, -0.0172,  ..., -0.0236,  0.0114,  0.0265],\n",
       "                       ...,\n",
       "                       [-0.0323,  0.0177, -0.0185,  ...,  0.0203, -0.0294,  0.0128],\n",
       "                       [ 0.0005, -0.0061, -0.0192,  ..., -0.0012, -0.0228, -0.0126],\n",
       "                       [-0.0292,  0.0084,  0.0341,  ...,  0.0029,  0.0039,  0.0366]])),\n",
       "              ('tst_decoder.tst_decoder.bias_ih_l1_reverse',\n",
       "               tensor([-0.0035, -0.0086,  0.0168,  ..., -0.0172,  0.0088, -0.0150])),\n",
       "              ('tst_decoder.tst_decoder.bias_hh_l1_reverse',\n",
       "               tensor([-0.0297, -0.0595, -0.0205,  ..., -0.0126,  0.0130,  0.0026])),\n",
       "              ('tst_decoder.fc.weight',\n",
       "               tensor([[-0.0115,  0.0067, -0.0309,  ...,  0.0466, -0.0137,  0.0227],\n",
       "                       [-0.0452,  0.0368, -0.0486,  ...,  0.0557, -0.0468,  0.0184],\n",
       "                       [ 0.0225, -0.0065,  0.0260,  ..., -0.0311, -0.0282,  0.0009],\n",
       "                       ...,\n",
       "                       [-0.0021,  0.0221, -0.0488,  ...,  0.0342, -0.0351,  0.0137],\n",
       "                       [-0.0208,  0.0442, -0.0291,  ...,  0.0475, -0.0166,  0.0166],\n",
       "                       [-0.0461,  0.0354, -0.0152,  ...,  0.0393, -0.0342, -0.0239]])),\n",
       "              ('tst_decoder.fc.bias',\n",
       "               tensor([-0.0488, -0.0429,  0.0145,  ..., -0.0222, -0.0422, -0.0168])),\n",
       "              ('half_hidden.weight',\n",
       "               tensor([[-0.1362,  0.4380, -0.0070,  0.2422],\n",
       "                       [ 0.1166, -0.4475,  0.2472,  0.1184]])),\n",
       "              ('half_hidden.bias', tensor([-0.0083,  0.0596])),\n",
       "              ('content2mean.weight',\n",
       "               tensor([[-0.0080, -0.0023, -0.0007,  ..., -0.0284, -0.0324, -0.0140],\n",
       "                       [-0.0324,  0.0140, -0.0096,  ...,  0.0268,  0.0083, -0.0183],\n",
       "                       [ 0.0460,  0.0281,  0.0436,  ...,  0.0366,  0.0057, -0.0223],\n",
       "                       ...,\n",
       "                       [ 0.0105, -0.0121, -0.0057,  ..., -0.0094,  0.0240,  0.0166],\n",
       "                       [-0.0141,  0.0319, -0.0378,  ...,  0.0047, -0.0461,  0.0030],\n",
       "                       [-0.0081,  0.0250,  0.0440,  ...,  0.0315,  0.0426,  0.0197]])),\n",
       "              ('content2mean.bias',\n",
       "               tensor([ 0.0410,  0.0351,  0.0206,  ..., -0.0291, -0.0164,  0.0153])),\n",
       "              ('content2logv.weight',\n",
       "               tensor([[ 0.0178,  0.0566, -0.0294,  ...,  0.0177,  0.0021,  0.0124],\n",
       "                       [ 0.0039,  0.0056, -0.0093,  ...,  0.0052,  0.0218,  0.0072],\n",
       "                       [-0.0246,  0.0133, -0.0586,  ..., -0.0218, -0.0103, -0.0086],\n",
       "                       ...,\n",
       "                       [-0.0635,  0.0170, -0.0225,  ..., -0.0223, -0.0037,  0.0319],\n",
       "                       [ 0.0142,  0.0217, -0.0468,  ...,  0.0192,  0.0249, -0.0059],\n",
       "                       [ 0.0050,  0.0042, -0.0586,  ...,  0.0293,  0.0060,  0.0255]])),\n",
       "              ('content2logv.bias',\n",
       "               tensor([0.0219, 0.0336, 0.0162,  ..., 0.0031, 0.0290, 0.0293])),\n",
       "              ('style2mean.weight',\n",
       "               tensor([[-0.0190,  0.0481, -0.0053,  ...,  0.0533, -0.0309,  0.0446],\n",
       "                       [ 0.0258, -0.0397, -0.0633,  ..., -0.0020,  0.0464, -0.0026],\n",
       "                       [-0.0053, -0.0121,  0.0466,  ...,  0.0410, -0.0021,  0.0181],\n",
       "                       ...,\n",
       "                       [ 0.0369, -0.0225,  0.0034,  ...,  0.0180,  0.0022,  0.0007],\n",
       "                       [ 0.0531,  0.0152,  0.0536,  ..., -0.0092,  0.0303,  0.0268],\n",
       "                       [ 0.0452, -0.0510, -0.0289,  ..., -0.0211, -0.0359,  0.0297]])),\n",
       "              ('style2mean.bias',\n",
       "               tensor([-0.0173,  0.0306, -0.0347,  ...,  0.0005, -0.0151,  0.0334])),\n",
       "              ('style2logv.weight',\n",
       "               tensor([[ 0.0199, -0.0503,  0.0228,  ..., -0.0127, -0.0174,  0.0382],\n",
       "                       [-0.0462, -0.0100, -0.0083,  ...,  0.0440, -0.0071, -0.0056],\n",
       "                       [-0.0493, -0.0302, -0.0215,  ..., -0.0379, -0.0340,  0.0271],\n",
       "                       ...,\n",
       "                       [ 0.0018, -0.0139,  0.0032,  ..., -0.0084,  0.0079,  0.0473],\n",
       "                       [-0.0592,  0.0030, -0.0069,  ...,  0.0361, -0.0232, -0.0677],\n",
       "                       [ 0.0525, -0.0499, -0.0248,  ..., -0.0547, -0.0279, -0.0476]])),\n",
       "              ('style2logv.bias',\n",
       "               tensor([ 0.0233, -0.0170, -0.0191,  ..., -0.0302,  0.0212,  0.0378]))]),\n",
       " 'encoder': OrderedDict([('src_embedding.weight',\n",
       "               tensor([[ 0.2827,  0.0358,  0.5101,  ...,  0.7153, -0.3080, -1.2164],\n",
       "                       [-1.5855, -0.7134, -1.4938,  ..., -1.3202, -0.1101,  1.6480],\n",
       "                       [-0.0306,  0.3408,  0.4493,  ..., -0.8764,  0.0742,  0.0642],\n",
       "                       ...,\n",
       "                       [-1.4204,  0.5075, -0.1172,  ..., -0.3030,  1.2276,  0.1671],\n",
       "                       [-0.7858, -0.6558,  0.7487,  ..., -2.0158, -0.4633,  0.5521],\n",
       "                       [-1.1546, -0.8147, -0.0947,  ..., -0.2383, -1.4264,  0.5645]])),\n",
       "              ('encoder.weight_ih_l0',\n",
       "               tensor([[-0.0419, -0.0469, -0.0271,  ...,  0.0011,  0.0503,  0.0204],\n",
       "                       [ 0.0112, -0.0291,  0.0083,  ...,  0.0113, -0.0264,  0.0207],\n",
       "                       [ 0.0178,  0.0250, -0.0021,  ...,  0.0339, -0.0029,  0.0152],\n",
       "                       ...,\n",
       "                       [ 0.0917,  0.0614,  0.0323,  ...,  0.0208,  0.0350, -0.0025],\n",
       "                       [ 0.0216,  0.0163,  0.0107,  ...,  0.0154,  0.0132, -0.0065],\n",
       "                       [-0.0724,  0.0118,  0.0004,  ...,  0.0278,  0.0185, -0.0117]])),\n",
       "              ('encoder.weight_hh_l0',\n",
       "               tensor([[-0.0136,  0.0130, -0.0144,  ..., -0.0111,  0.0213, -0.0128],\n",
       "                       [-0.0235,  0.0193, -0.0309,  ..., -0.0232, -0.0244, -0.0429],\n",
       "                       [ 0.0311,  0.0109, -0.0153,  ...,  0.0136, -0.0451,  0.0405],\n",
       "                       ...,\n",
       "                       [ 0.0096,  0.0234, -0.0037,  ..., -0.0292, -0.0086,  0.0056],\n",
       "                       [-0.0043,  0.0109, -0.0248,  ..., -0.0181,  0.0109, -0.0210],\n",
       "                       [ 0.0335, -0.0037,  0.0222,  ...,  0.0228,  0.0136,  0.0477]])),\n",
       "              ('encoder.bias_ih_l0',\n",
       "               tensor([-0.0550, -0.0492, -0.0223,  ..., -0.0142, -0.0089, -0.0228])),\n",
       "              ('encoder.bias_hh_l0',\n",
       "               tensor([-0.0384, -0.0368, -0.0314,  ...,  0.0096, -0.0099, -0.0107])),\n",
       "              ('encoder.weight_ih_l0_reverse',\n",
       "               tensor([[ 0.0120,  0.0361, -0.0190,  ...,  0.0340, -0.0059, -0.0071],\n",
       "                       [ 0.0428, -0.0168, -0.0980,  ...,  0.0360, -0.0401,  0.0417],\n",
       "                       [ 0.0407, -0.0020, -0.0708,  ...,  0.0731,  0.0614, -0.0541],\n",
       "                       ...,\n",
       "                       [-0.0021, -0.0155,  0.0157,  ...,  0.0239,  0.0060, -0.0180],\n",
       "                       [ 0.0099,  0.0207,  0.0220,  ...,  0.0216,  0.0375,  0.0028],\n",
       "                       [-0.0059,  0.0089,  0.0298,  ...,  0.0094, -0.0140, -0.0185]])),\n",
       "              ('encoder.weight_hh_l0_reverse',\n",
       "               tensor([[ 0.0340, -0.0462, -0.0178,  ..., -0.0139,  0.0316,  0.0019],\n",
       "                       [-0.0232,  0.0305, -0.0344,  ...,  0.0777,  0.0557,  0.0466],\n",
       "                       [-0.0581, -0.0437,  0.0190,  ..., -0.0198, -0.0479, -0.0459],\n",
       "                       ...,\n",
       "                       [-0.0331, -0.0147,  0.0155,  ..., -0.0384, -0.0338,  0.0054],\n",
       "                       [-0.0213,  0.0354, -0.0163,  ...,  0.0186, -0.0515,  0.0220],\n",
       "                       [ 0.0006, -0.0068, -0.0206,  ..., -0.0617,  0.0005, -0.0269]])),\n",
       "              ('encoder.bias_ih_l0_reverse',\n",
       "               tensor([-0.0344, -0.0359, -0.0461,  ..., -0.0155, -0.0374,  0.0087])),\n",
       "              ('encoder.bias_hh_l0_reverse',\n",
       "               tensor([-0.0824, -0.0082, -0.0265,  ...,  0.0048,  0.0043,  0.0056])),\n",
       "              ('encoder.weight_ih_l1',\n",
       "               tensor([[-0.0158, -0.0168, -0.0018,  ...,  0.0085, -0.0079,  0.0145],\n",
       "                       [-0.0019, -0.0099,  0.0287,  ..., -0.0294,  0.0203,  0.0207],\n",
       "                       [ 0.0111,  0.0368,  0.0205,  ..., -0.0150, -0.0243,  0.0167],\n",
       "                       ...,\n",
       "                       [-0.0305, -0.0158, -0.0236,  ..., -0.0098, -0.0031,  0.0042],\n",
       "                       [ 0.0185,  0.0157, -0.0082,  ...,  0.0316,  0.0257, -0.0099],\n",
       "                       [-0.0263,  0.0256,  0.0068,  ..., -0.0071,  0.0301, -0.0402]])),\n",
       "              ('encoder.weight_hh_l1',\n",
       "               tensor([[-0.0213, -0.0171, -0.0031,  ...,  0.0357, -0.0159, -0.0019],\n",
       "                       [-0.0026, -0.0063, -0.0099,  ...,  0.0326, -0.0126,  0.0196],\n",
       "                       [ 0.0202,  0.0348, -0.0201,  ..., -0.0258,  0.0343, -0.0024],\n",
       "                       ...,\n",
       "                       [ 0.0269, -0.0027, -0.0020,  ...,  0.0209, -0.0083,  0.0041],\n",
       "                       [ 0.0163, -0.0080,  0.0072,  ..., -0.0270, -0.0080, -0.0142],\n",
       "                       [-0.0091, -0.0198, -0.0033,  ...,  0.0165,  0.0215, -0.0087]])),\n",
       "              ('encoder.bias_ih_l1',\n",
       "               tensor([-0.0332, -0.0279, -0.0335,  ...,  0.0176,  0.0282,  0.0135])),\n",
       "              ('encoder.bias_hh_l1',\n",
       "               tensor([ 0.0084,  0.0038,  0.0159,  ..., -0.0316,  0.0230,  0.0061])),\n",
       "              ('encoder.weight_ih_l1_reverse',\n",
       "               tensor([[ 0.0884,  0.0669, -0.0036,  ..., -0.0132,  0.0132, -0.0247],\n",
       "                       [ 0.0525, -0.0040,  0.0121,  ..., -0.0300,  0.0306, -0.0292],\n",
       "                       [ 0.0207,  0.0148,  0.0453,  ..., -0.0218, -0.0055, -0.0339],\n",
       "                       ...,\n",
       "                       [-0.0214, -0.0473,  0.0309,  ..., -0.0300,  0.0033, -0.0348],\n",
       "                       [-0.0662, -0.0537,  0.0654,  ..., -0.0090, -0.0028,  0.0030],\n",
       "                       [-0.0499, -0.0530,  0.0104,  ..., -0.0035,  0.0018, -0.0090]])),\n",
       "              ('encoder.weight_hh_l1_reverse',\n",
       "               tensor([[-0.0105, -0.0056,  0.0168,  ..., -0.0225,  0.0179, -0.0064],\n",
       "                       [-0.0278,  0.0441, -0.0769,  ...,  0.0023, -0.0019,  0.0139],\n",
       "                       [-0.0300,  0.0285, -0.0105,  ...,  0.0216, -0.0046, -0.0097],\n",
       "                       ...,\n",
       "                       [-0.0173, -0.0342, -0.0260,  ..., -0.0209, -0.0124, -0.0278],\n",
       "                       [-0.0253,  0.0017, -0.0336,  ...,  0.0054, -0.0505, -0.0377],\n",
       "                       [-0.0132, -0.0025, -0.0100,  ...,  0.0275,  0.0086, -0.0187]])),\n",
       "              ('encoder.bias_ih_l1_reverse',\n",
       "               tensor([ 0.0143, -0.0287,  0.0167,  ...,  0.0054,  0.0008, -0.0420])),\n",
       "              ('encoder.bias_hh_l1_reverse',\n",
       "               tensor([ 0.0014, -0.0209,  0.0077,  ...,  0.0018, -0.0097, -0.0246]))]),\n",
       " 'total_latent': tensor([[[ 4.7028, -1.0343,  1.5588,  ..., -0.5246,  3.8837, -0.4773],\n",
       "          [-2.3886,  0.4278,  0.1479,  ..., -0.5428, -3.0991, -0.6043],\n",
       "          [-1.4048,  1.2207, -0.7837,  ..., -2.0178, -4.0063, -0.9412],\n",
       "          ...,\n",
       "          [-0.5096, -0.2311, -2.9334,  ...,  0.5223,  0.2022,  0.0444],\n",
       "          [ 4.2281,  0.2994, -0.0555,  ..., -0.6254, -1.1648,  0.1028],\n",
       "          [-1.7475,  0.9391, -1.9290,  ..., -0.8280, -1.5146, -0.7290]],\n",
       " \n",
       "         [[-0.8868,  1.4147,  1.5940,  ..., -0.9829, -0.3294,  2.9508],\n",
       "          [-0.4221, -1.7186,  0.1074,  ..., -0.1578,  0.5597,  4.7185],\n",
       "          [-0.1725,  1.1337,  1.1389,  ..., -1.0782,  0.4285,  4.2885],\n",
       "          ...,\n",
       "          [ 0.2874,  1.0019,  0.6523,  ..., -0.6761,  0.3053, -1.7260],\n",
       "          [-0.2885,  2.0189,  0.7984,  ...,  0.1334, -0.1141,  0.4187],\n",
       "          [-0.2783,  1.2636,  1.7720,  ..., -0.1499,  0.2317, -0.7682]],\n",
       " \n",
       "         [[-0.3126, -1.5161,  0.0366,  ..., -0.0161, -0.7948, -0.0285],\n",
       "          [-0.3719, -0.3572, -1.5249,  ...,  0.3471,  0.7703,  0.1877],\n",
       "          [ 0.8363,  0.6800,  1.7260,  ..., -0.9716, -0.9358, -2.1614],\n",
       "          ...,\n",
       "          [-1.0105,  0.0624,  0.8795,  ..., -1.2660, -0.2748,  0.2612],\n",
       "          [ 0.8988, -0.3813,  0.3035,  ...,  1.0241,  0.8664,  1.6027],\n",
       "          [-0.0418, -2.0030, -0.0986,  ..., -1.2970, -1.1234, -0.3643]],\n",
       " \n",
       "         [[-0.2329, -2.0684, -0.6538,  ...,  1.0418,  1.7692, -0.5550],\n",
       "          [ 0.5913, -0.7814,  0.6220,  ..., -0.7936, -2.7161, -1.2859],\n",
       "          [ 0.8517, -0.7660, -0.7701,  ...,  0.0382, -1.1991,  0.5635],\n",
       "          ...,\n",
       "          [ 0.3513, -0.9231, -0.6516,  ...,  2.4126,  1.0073,  0.3181],\n",
       "          [-1.6104, -1.6971,  0.3098,  ...,  2.4479,  0.7057,  0.2434],\n",
       "          [-0.3710, -0.2161, -0.6142,  ...,  0.8752, -0.1490, -0.0995]]],\n",
       "        requires_grad=True)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_model = t(torch.load(\"../../kcc_data/tst_model.pth\", map_location=torch.device('cpu')))\n",
    "tst_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setting\n",
    "with open(\"../data/processed/tokenized/spm_tokenized_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "em_informal_train = data[\"gyafc\"][\"train\"][\"em_informal\"]\n",
    "em_informal_test = data[\"gyafc\"][\"train\"][\"em_informal\"]\n",
    "em_formal_train = data[\"gyafc\"][\"train\"][\"em_formal\"]\n",
    "pair_kor_train = data['korpora']['train']['pair_kor']\n",
    "pair_eng_train = data['korpora']['train']['pair_eng']\n",
    "# fr_informal_train = data[\"gyafc\"][\"train\"][\"fr_informal\"]\n",
    "# fr_formal_train = data[\"gyafc\"][\"train\"][\"fr_formal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5259, 52595, 52070, 5207)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-int(len(data[\"gyafc\"][\"train\"][\"em_informal\"])*0.1), len(data[\"gyafc\"][\"train\"][\"em_informal\"]), len(em_informal_train), len(em_informal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5259"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(data[\"gyafc\"][\"train\"][\"em_informal\"])*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "em_informal_train = em_informal_train[:int(len(em_informal_train) * split_ratio)]\n",
    "em_informal_valid = em_informal_train[int(len(em_informal_train) * split_ratio):]\n",
    "em_formal_train = em_formal_train[:int(len(em_formal_train) * split_ratio)]\n",
    "em_formal_valid = em_formal_train[int(len(em_formal_train) * split_ratio):]\n",
    "pair_kor_train = pair_kor_train[:int(len(pair_kor_train) * split_ratio)]\n",
    "pair_kor_valid = pair_kor_train[int(len(pair_kor_train) * split_ratio):]\n",
    "pair_eng_train = pair_eng_train[:int(len(pair_eng_train) * split_ratio)]\n",
    "pair_eng_valid = pair_eng_train[int(len(pair_eng_train) * split_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tokenizers\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_parallel] download korean-english-park.train.tar.gz: 8.72MB [00:13, 656kB/s]                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompress C:\\workspace\\kcc\\kor_dataset\\korean_parallel\\korean-english-park.train.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_parallel] download korean-english-park.dev.tar.gz: 115kB [00:00, 574kB/s]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompress C:\\workspace\\kcc\\kor_dataset\\korean_parallel\\korean-english-park.dev.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_parallel] download korean-english-park.test.tar.gz: 238kB [00:00, 1.10MB/s]                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompress C:\\workspace\\kcc\\kor_dataset\\korean_parallel\\korean-english-park.test.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_path = \"../kor_dataset/\"\n",
    "Korpora.fetch(\"korean_parallel_koen_news\", root_dir=download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : KakaoBrain\n",
      "    Repository : https://github.com/jungyeul/korean-parallel-corpora\n",
      "    References :\n",
      "        - Jungyeul Park, Jeen-Pyo Hong and Jeong-Won Cha (2016) Korean Language Resources for Everyone.\n",
      "          In Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation\n",
      "          (PACLIC 30). October 28 - 30, 2016. Seoul, Korea. \n",
      "          (https://www.aclweb.org/anthology/Y16-2002/)\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution Noncommercial No-Derivative-Works 3.0\n",
      "    Details in https://creativecommons.org/licenses/by-nc-nd/3.0/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Korpora import KoreanParallelKOENNewsKorpus\n",
    "corpus = KoreanParallelKOENNewsKorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor = corpus.test.get_all_texts()\n",
    "eng = corpus.test.get_all_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/raw/korpora/pair_kor_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in kor:\n",
    "        f.write(f\"{line}\\n\")\n",
    "with open(\"../data/processed/raw/korpora/pair_eng_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in eng:\n",
    "        f.write(f\"{line}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make spm model of each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"./data/processed/raw/kopora/*{data_type}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Compelte: train pair_eng model & vocab\n",
      "Make Compelte: train pair_eng tokenized data\n",
      "Train Compelte: train pair_kor model & vocab\n",
      "Make Compelte: train pair_kor tokenized data\n",
      "Train Compelte: test pair_eng model & vocab\n",
      "Make Compelte: test pair_eng tokenized data\n",
      "Train Compelte: test pair_kor model & vocab\n",
      "Make Compelte: test pair_kor tokenized data\n",
      "Saving Tokenized Data is Done!\n",
      "Train Compelte: train em_formal model & vocab\n",
      "Make Compelte: train em_formal tokenized data\n",
      "Train Compelte: train em_informal model & vocab\n",
      "Make Compelte: train em_informal tokenized data\n",
      "Train Compelte: train fr_formal model & vocab\n",
      "Make Compelte: train fr_formal tokenized data\n",
      "Train Compelte: train fr_informal model & vocab\n",
      "Make Compelte: train fr_informal tokenized data\n",
      "Train Compelte: test em_formal model & vocab\n",
      "Make Compelte: test em_formal tokenized data\n",
      "Train Compelte: test em_informal model & vocab\n",
      "Make Compelte: test em_informal tokenized data\n",
      "Train Compelte: test fr_formal model & vocab\n",
      "Make Compelte: test fr_formal tokenized data\n",
      "Train Compelte: test fr_informal model & vocab\n",
      "Make Compelte: test fr_informal tokenized data\n",
      "Saving Tokenized Data is Done!\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = {}\n",
    "for corpus in [\"korpora\", \"gyafc\"]:\n",
    "    tokenized_data[corpus] = {}\n",
    "    tokenized_data[corpus]['train'] = {}\n",
    "    tokenized_data[corpus]['test'] = {}\n",
    "\n",
    "    for data_type in [\"train\", \"test\"]:\n",
    "        files = glob.glob(f\"./data/processed/raw/{corpus}/*{data_type}.txt\")\n",
    "        \n",
    "        parameter = '--input={} \\\n",
    "        --pad_id={} --pad_piece={} \\\n",
    "        --bos_id={} --bos_piece={} \\\n",
    "        --eos_id={} --eos_piece={} \\\n",
    "        --unk_id={} --unk_piece={} \\\n",
    "        --user_defined_symbols={} \\\n",
    "        --model_prefix={} \\\n",
    "        --vocab_size={} \\\n",
    "        --max_sentence_length={} \\\n",
    "        --character_coverage={} \\\n",
    "        --model_type={}'\n",
    "\n",
    "        pad_id = 0\n",
    "        pad_piece = \"[PAD]\"\n",
    "        bos_id = 1\n",
    "        bos_piece = \"[BOS]\"\n",
    "        eos_id = 2\n",
    "        eos_piece = \"[EOS]\"\n",
    "        unk_id = 3\n",
    "        unk_piece = \"[UNK]\"\n",
    "        user_defined_symbols = \"[SEP],[CLS],[MASK]\"\n",
    "        if corpus == \"korpora\":\n",
    "            vocab_size = 2400\n",
    "        elif corpus == \"gyafc\":\n",
    "            vocab_size = 1800\n",
    "        max_sentence_length = 9999\n",
    "        character_coverage = 1.0  # default\n",
    "        model_type = 'unigram'  # default: unigram\n",
    "\n",
    "        for train_input_file in files:        \n",
    "            prefix = ((\"_\").join(train_input_file.split(\"\\\\\")[-1].split(\"_\")[:-1]))\n",
    "            model_prefix = f'./data/tokenizer/{data_type}_{prefix}_spm'\n",
    "\n",
    "            cmd = parameter.format(train_input_file,\n",
    "                                   pad_id, pad_piece,\n",
    "                                   bos_id, bos_piece,\n",
    "                                   eos_id, eos_piece,\n",
    "                                   unk_id, unk_piece,\n",
    "                                   user_defined_symbols,\n",
    "                                   model_prefix,\n",
    "                                   vocab_size,\n",
    "                                   max_sentence_length,\n",
    "                                   character_coverage,\n",
    "                                   model_type)\n",
    "            spm.SentencePieceProcessor()\n",
    "            spm.SentencePieceTrainer.Train(cmd)\n",
    "            print(f\"Train Compelte: {data_type} {prefix} model & vocab\")\n",
    "\n",
    "            sp = spm.SentencePieceProcessor()\n",
    "            sp.Load(f\"{model_prefix}.model\")\n",
    "\n",
    "            # BOS, EOS 추가\n",
    "            sp.SetEncodeExtraOptions('bos:eos')\n",
    "\n",
    "            # Tokenization And Padding\n",
    "            with open(train_input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                tokenized_data[corpus][data_type][prefix] = [sp.EncodeAsIds(line) for line in f]\n",
    "                print(f\"Make Compelte: {data_type} {prefix} tokenized data\")\n",
    "\n",
    "    # Save Data\n",
    "    processed_path = \"./data/processed/tokenized/spm_tokenized_data.pkl\"\n",
    "    with open(processed_path, 'wb') as file:\n",
    "        pickle.dump(tokenized_data, file)\n",
    "    print(\"Saving Tokenized Data is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['korpora', 'gyafc'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__builtin__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-21a429f2ae3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../.data/pkl/aihub_spm_bpe.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '__builtin__'"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(\"../.data/pkl/aihub_spm_bpe.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['em_formal'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
